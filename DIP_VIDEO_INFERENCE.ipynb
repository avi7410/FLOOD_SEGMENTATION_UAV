{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HbUD5SzwRFSI",
        "outputId": "90b1bb0a-d18c-413e-f5e6-bdb81f7a4845"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X-ejA9fjS4up"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "import torch\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from torchvision import transforms\n",
        "\n",
        "import torch\n",
        "import argparse\n",
        "import os\n",
        "import cv2\n",
        "from PIL import Image\n",
        "from IPython.display import display\n",
        "\n",
        "import cv2\n",
        "import torch\n",
        "import time\n",
        "import os\n",
        "from PIL import Image\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MGCrZ-VVS2MJ"
      },
      "source": [
        "# CONFIG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1qxZCnIsS3C2"
      },
      "outputs": [],
      "source": [
        "ALL_CLASSES = ['background', 'flood']\n",
        "\n",
        "LABEL_COLORS_LIST = [\n",
        "    (0, 0, 0), # Background.\n",
        "    (255, 255, 255), # Flood.\n",
        "]\n",
        "\n",
        "VIS_LABEL_MAP = [\n",
        "    (0, 0, 0), # Background.\n",
        "    (255, 0, 0), # Flood.\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qvpoxWTbT40d"
      },
      "source": [
        "# MODEL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nhj2Qr6ZT56_"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "#from torchvision.models.segmentation import deeplabv3_resnet50\n",
        "\n",
        "#def prepare_model(num_classes=2):\n",
        "#    model = deeplabv3_resnet50(weights='DEFAULT')\n",
        "#    model.classifier[4] = nn.Conv2d(256, num_classes, 1)\n",
        "#    model.aux_classifier[4] = nn.Conv2d(256, num_classes, 1)\n",
        "#    return model\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Uncomment the following lines to train on the DeepLabV3 ResNet101 model.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "from torchvision.models.segmentation import deeplabv3_resnet101\n",
        "\n",
        "def prepare_model(num_classes=2):\n",
        "    model = deeplabv3_resnet101(weights='DEFAULT')\n",
        "    model.classifier[4] = nn.Conv2d(256, num_classes, 1)\n",
        "    model.aux_classifier[4] = nn.Conv2d(256, num_classes, 1)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VL58rFkhSwvd"
      },
      "source": [
        "# UTILS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HUG7xIdTSwLV"
      },
      "outputs": [],
      "source": [
        "plt.style.use('ggplot')\n",
        "\n",
        "def set_class_values(all_classes, classes_to_train):\n",
        "    \"\"\"\n",
        "    This (`class_values`) assigns a specific class label to the each of the classes.\n",
        "    For example, `animal=0`, `archway=1`, and so on.\n",
        "\n",
        "    :param all_classes: List containing all class names.\n",
        "    :param classes_to_train: List containing class names to train.\n",
        "    \"\"\"\n",
        "    class_values = [all_classes.index(cls.lower()) for cls in classes_to_train]\n",
        "    return class_values\n",
        "\n",
        "def get_label_mask(mask, class_values, label_colors_list):\n",
        "    \"\"\"\n",
        "    This function encodes the pixels belonging to the same class\n",
        "    in the image into the same label\n",
        "\n",
        "    :param mask: NumPy array, segmentation mask.\n",
        "    :param class_values: List containing class values, e.g car=0, bus=1.\n",
        "    :param label_colors_list: List containing RGB color value for each class.\n",
        "    \"\"\"\n",
        "    label_mask = np.zeros((mask.shape[0], mask.shape[1]), dtype=np.uint8)\n",
        "    for value in class_values:\n",
        "        for ii, label in enumerate(label_colors_list):\n",
        "            if value == label_colors_list.index(label):\n",
        "                label = np.array(label)\n",
        "                label_mask[np.where(np.all(mask == label, axis=-1))[:2]] = value\n",
        "    label_mask = label_mask.astype(int)\n",
        "    return label_mask\n",
        "\n",
        "def draw_translucent_seg_maps(\n",
        "    data,\n",
        "    output,\n",
        "    epoch,\n",
        "    i,\n",
        "    val_seg_dir,\n",
        "    label_colors_list,\n",
        "):\n",
        "    \"\"\"\n",
        "    This function color codes the segmentation maps that are generated while\n",
        "    validating. THIS IS NOT TO BE CALLED FOR SINGLE IMAGE TESTING\n",
        "    \"\"\"\n",
        "    alpha = 1  # how much transparency\n",
        "    beta = 0.6  # alpha + beta should be 1\n",
        "    gamma = 0  # contrast\n",
        "\n",
        "    seg_map = output[0]  # use only one output from the batch\n",
        "    seg_map = torch.argmax(seg_map.squeeze(), dim=0).detach().cpu().numpy()\n",
        "\n",
        "    image = data[0]\n",
        "    image = np.array(image.cpu())\n",
        "    image = np.transpose(image, (1, 2, 0))\n",
        "\n",
        "    red_map = np.zeros_like(seg_map).astype(np.uint8)\n",
        "    green_map = np.zeros_like(seg_map).astype(np.uint8)\n",
        "    blue_map = np.zeros_like(seg_map).astype(np.uint8)\n",
        "\n",
        "    for label_num in range(0, len(label_colors_list)):\n",
        "        index = seg_map == label_num\n",
        "        red_map[index] = label_colors_list[label_num][0]\n",
        "        green_map[index] = label_colors_list[label_num][1]\n",
        "        blue_map[index] = label_colors_list[label_num][2]\n",
        "\n",
        "    rgb = np.stack([red_map, green_map, blue_map], axis=2)\n",
        "    rgb = np.array(rgb, dtype=np.float32)\n",
        "    # convert color to BGR format for OpenCV\n",
        "    rgb = cv2.cvtColor(rgb, cv2.COLOR_RGB2BGR)\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
        "    cv2.addWeighted(image, alpha, rgb, beta, gamma, image)\n",
        "    cv2.imwrite(f\"{val_seg_dir}/e{epoch}_b{i}.jpg\", image)\n",
        "\n",
        "\n",
        "class SaveBestModel:\n",
        "    \"\"\"\n",
        "    Class to save the best model while training. If the current epoch's\n",
        "    validation loss is less than the previous least less, then save the\n",
        "    model state.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self, best_valid_loss=float('inf')\n",
        "    ):\n",
        "        self.best_valid_loss = best_valid_loss\n",
        "\n",
        "    def __call__(\n",
        "        self, current_valid_loss, epoch, model, out_dir, name='model'\n",
        "    ):\n",
        "        if current_valid_loss < self.best_valid_loss:\n",
        "            self.best_valid_loss = current_valid_loss\n",
        "            print(f\"\\nBest validation loss: {self.best_valid_loss}\")\n",
        "            print(f\"\\nSaving best model for epoch: {epoch+1}\\n\")\n",
        "            torch.save({\n",
        "                'epoch': epoch+1,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                }, os.path.join(out_dir, 'best_'+name+'.pth'))\n",
        "\n",
        "def save_model(epochs, model, optimizer, criterion, out_dir, name='model'):\n",
        "    \"\"\"\n",
        "    Function to save the trained model to disk.\n",
        "    \"\"\"\n",
        "    torch.save({\n",
        "                'epoch': epochs,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'loss': criterion,\n",
        "                }, os.path.join(out_dir, name+'.pth'))\n",
        "\n",
        "def save_plots(train_acc, valid_acc, train_loss, valid_loss, out_dir):\n",
        "    \"\"\"\n",
        "    Function to save the loss and accuracy plots to disk.\n",
        "    \"\"\"\n",
        "    # Accuracy plots.\n",
        "    plt.figure(figsize=(10, 7))\n",
        "    plt.plot(\n",
        "        train_acc, color='tab:blue', linestyle='-',\n",
        "        label='train accuracy'\n",
        "    )\n",
        "    plt.plot(\n",
        "        valid_acc, color='tab:red', linestyle='-',\n",
        "        label='validataion accuracy'\n",
        "    )\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "    plt.savefig(os.path.join(out_dir, 'accuracy.png'))\n",
        "\n",
        "    # Loss plots.\n",
        "    plt.figure(figsize=(10, 7))\n",
        "    plt.plot(\n",
        "        train_loss, color='tab:blue', linestyle='-',\n",
        "        label='train loss'\n",
        "    )\n",
        "    plt.plot(\n",
        "        valid_loss, color='tab:red', linestyle='-',\n",
        "        label='validataion loss'\n",
        "    )\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.savefig(os.path.join(out_dir, 'loss.png'))\n",
        "\n",
        "# Define the torchvision image transforms\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "def get_segment_labels(image, model, device):\n",
        "    # transform the image to tensor and load into computation device\n",
        "    image = transform(image).to(device)\n",
        "    image = image.unsqueeze(0) # add a batch dimension\n",
        "    with torch.no_grad():\n",
        "        outputs = model(image)\n",
        "    return outputs\n",
        "\n",
        "def draw_segmentation_map(outputs):\n",
        "    labels = torch.argmax(outputs.squeeze(), dim=0).detach().cpu().numpy()\n",
        "\n",
        "    # create Numpy arrays containing zeros\n",
        "    # later to be used to fill them with respective red, green, and blue pixels\n",
        "    red_map = np.zeros_like(labels).astype(np.uint8)\n",
        "    green_map = np.zeros_like(labels).astype(np.uint8)\n",
        "    blue_map = np.zeros_like(labels).astype(np.uint8)\n",
        "\n",
        "    for label_num in range(0, len(VIS_LABEL_MAP)):\n",
        "        index = labels == label_num\n",
        "        red_map[index] = np.array(VIS_LABEL_MAP)[label_num, 0]\n",
        "        green_map[index] = np.array(VIS_LABEL_MAP)[label_num, 1]\n",
        "        blue_map[index] = np.array(VIS_LABEL_MAP)[label_num, 2]\n",
        "\n",
        "    segmentation_map = np.stack([red_map, green_map, blue_map], axis=2)\n",
        "    return segmentation_map\n",
        "\n",
        "def image_overlay(image, segmented_image):\n",
        "    alpha = 1 # transparency for the original image\n",
        "    beta = 1.0 # transparency for the segmentation map\n",
        "    gamma = 0 # scalar added to each sum\n",
        "\n",
        "    segmented_image = cv2.cvtColor(segmented_image, cv2.COLOR_RGB2BGR)\n",
        "    image = np.array(image)\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
        "    cv2.addWeighted(image, alpha, segmented_image, beta, gamma, image)\n",
        "    return image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NTnqpmEpgjgl"
      },
      "source": [
        "# VIDEO INFERENCE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "# Assign input video file path directly\n",
        "input_video_path = '/content/drive/MyDrive/flood-area-segmentation/video2.mp4'\n",
        "\n",
        "out_dir = '/content/drive/MyDrive/flood-area-segmentation/outputs/'\n",
        "os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "# Set computation device.\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model = prepare_model(len(ALL_CLASSES))\n",
        "ckpt = torch.load('/content/drive/MyDrive/flood-area-segmentation/outputs/DeepLabV3_ResNet101.pth')\n",
        "model.load_state_dict(ckpt['model_state_dict'])\n",
        "model.eval().to(device)\n",
        "\n",
        "cap = cv2.VideoCapture(input_video_path)\n",
        "if not cap.isOpened():\n",
        "    print('Error while trying to read video. Please check the path again.')\n",
        "\n",
        "# get the frame width and height\n",
        "frame_width = int(cap.get(3))\n",
        "frame_height = int(cap.get(4))\n",
        "\n",
        "save_name = f\"{input_video_path.split('/')[-1].split('.')[0]}\"\n",
        "# define codec and create VideoWriter object\n",
        "out = cv2.VideoWriter(f\"{out_dir}/{save_name}.mp4\",\n",
        "                      cv2.VideoWriter_fourcc(*'mp4v'), 30,\n",
        "                      (frame_width, frame_height))\n",
        "\n",
        "frame_count = 0  # to count total frames\n",
        "total_fps = 0  # to get the final frames per second\n",
        "\n",
        "# read until the end of the video\n",
        "while cap.isOpened():\n",
        "    # capture each frame of the video\n",
        "    ret, frame = cap.read()\n",
        "    if ret:\n",
        "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "        # get the start time\n",
        "        start_time = time.time()\n",
        "        # Do forward pass and get the output dictionary.\n",
        "        outputs = get_segment_labels(rgb_frame, model, device)\n",
        "        # Get the data from the `out` key.\n",
        "        outputs = outputs['out']\n",
        "        segmented_image = draw_segmentation_map(outputs)\n",
        "\n",
        "        final_image = image_overlay(rgb_frame, segmented_image)\n",
        "\n",
        "        # get the end time\n",
        "        end_time = time.time()\n",
        "        # get the current fps\n",
        "        fps = 1 / (end_time - start_time)\n",
        "        # add the current fps to the total fps\n",
        "        total_fps += fps\n",
        "        # increment frame count\n",
        "        frame_count += 1\n",
        "        # put the FPS text on the current frame\n",
        "        cv2.putText(final_image, f\"{fps:.3f} FPS\", (20, 35),\n",
        "                    cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
        "        # press `q` to exit\n",
        "        cv2_imshow(final_image)\n",
        "        out.write(final_image)\n",
        "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "          break\n",
        "    else:\n",
        "        break\n",
        "\n",
        "# release VideoCapture()\n",
        "cap.release()\n",
        "# release VideoWriter()\n",
        "out.release()\n",
        "# close all frames and video windows\n",
        "cv2.destroyAllWindows()\n",
        "# calculate and print the average FPS\n",
        "avg_fps = total_fps / frame_count\n",
        "print(f\"Average FPS: {avg_fps:.3f}\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
